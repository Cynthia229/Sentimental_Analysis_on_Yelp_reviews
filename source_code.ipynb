{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of COMP4332 _Proj_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5ZbCQTRSwBB"
      },
      "source": [
        "# Fine-tune Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFZiNFMbITtV",
        "outputId": "59136d02-e029-4678-a99d-10b8604dc628"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 21.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=f5d6d9219eede4e3d058e0dcd44b44ec76c3b3cc7fed765e29ef65d5767f6655\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itQ6F_n6ILmD"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import re\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T83ib1exUuCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101634e4-cb0a-4428-f93e-125f169da66e"
      },
      "source": [
        "\n",
        "# use GPU if available\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('Use GPU')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q14NVW46HykG"
      },
      "source": [
        "def tokenize(text):\n",
        "\n",
        "    # try:\n",
        "    #     text = text.decode('utf-8').lower()\n",
        "    # except:\n",
        "    #     text = text.encode('utf-8').decode('utf-8').lower()\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(u\"\\u2019|\\u2018\", \"\\'\", text)\n",
        "    text = re.sub(u\"\\u201c|\\u201d\", \"\\\"\", text)\n",
        "    text = re.sub(r\"http[s]?:[^\\ ]+\", \" \", text)\n",
        "    text = re.sub(r\"&gt;\", \" \", text)\n",
        "    text = re.sub(r\"&lt;\", \" \", text)\n",
        "    text = re.sub(r\"&quot;\", \" \", text)\n",
        "    text = re.sub(r\"&nbsp;\", \" \", text)\n",
        "    text = re.sub(r\"via\", \" \", text)\n",
        "    text = re.sub(r\"#\\ \", \"#\", text)\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\\\r\", \" \", text)\n",
        "    text = re.sub(r\"\\\\\", \" \", text)\n",
        "    text = re.sub(r\"[\\(\\)\\[\\]\\{\\}]\", r\" \", text)\n",
        "    text = re.sub(r\"#\", \" #\", text)\n",
        "    text = re.sub(r\"\\@\", \" \", text)\n",
        "    text = re.sub(r\"[\\～\\+\\-\\《\\》\\、\\$\\%\\^\\>\\<\\=\\:\\;\\*\\(\\)\\{\\}\\[\\]\\/\\~\\&\\|\\【\\】]\", \" \", text)\n",
        "    text = ' '.join(text.split())\n",
        "    #words = text.split()\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moyOOt7NXJ0J"
      },
      "source": [
        "trainset = pd.read_csv('train2.csv')\n",
        "validset = pd.read_csv('valid2.csv')\n",
        "testset = pd.read_csv('test.csv')\n",
        "\n",
        "train_text = [tokenize(x) for x in trainset['text']]\n",
        "valid_text = [tokenize(x) for x in validset['text']]\n",
        "test_text = [tokenize(x) for x in testset['text']]\n",
        "\n",
        "train_label = list(trainset.stars-1)\n",
        "valid_label = list(validset.stars-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxvkfE5hZuIq",
        "outputId": "19e4e954-4aad-4e76-d744-b9eef318e737"
      },
      "source": [
        "from transformers import RobertaTokenizer\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading Roberta tokenizer...')\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Roberta tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qozTcghgaAhM",
        "outputId": "34f4ff17-4b26-495f-eda3-4a6369d12d41"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "train_input_ids = []\n",
        "# For every sentence...\n",
        "for sent in train_text:\n",
        "\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        \n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    train_input_ids.append(encoded_sent)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  \"what do you do for recreation?\" \"oh, the usual. i bowl. drive around. the occasional acid flashback.\" i do not actually like bowling. it's my nature. i only like things that i am good at and i am not even certain i would like to be good at bowling so i had my reservations when being invited to hang with the optimism club kids but i love these kids so... \"fuck it, dude, let's go bowling.\" i do have to admit i really had a blast here. it is vintage and scummy on the outside which to me is just a bowling alley being honest with itself. it looks so old and dingy from the outside that you are slightly taken aback by the automatic doors. it's a bit like you have never seen one before and wow isn't technology amazing? i have to say the bar service was a little slow. they rotated between three bartenders and 2 3 of them were ok so i will let it slide. the third on the other hand was probably better suited for my cube monkey, paper pushing job. the beers were however 2 for a domestic in a red dixie cup. it reminded me of the days i could get a red dixie cup foe 2 with free refills all night. ahh college days... we were here for cosmic bowling. it was somewhere around 12. i hadn't bowled in probably 5 or 6 years and i am pretty sure i paid twice that then. great prices. the dj was spinning some pretty excellent tunes hard drives spin so i can still say \"spin\" right? my favorite part of the evening went something like this. me \"this dj is playing some pretty decent stuff, but i think he has re played a few songs since we've been here. kinof lame.\" friend \"that's cedric ceballos.\" me \"what's cedric ceballos?\" friend \"the dj.\" me looks closer at dj \"holy shit the dj is cedric fucking ceballos!\" sunset bowl is pretty damned close to exactly what a bowling alley should be and remember \"this is not 'nam. this is bowling. there are rules.\"\n",
            "Token IDs: [0, 113, 12196, 109, 47, 109, 13, 14579, 1917, 22, 2678, 6, 5, 4505, 4, 939, 5749, 4, 1305, 198, 4, 5, 12577, 10395, 39765, 72, 939, 109, 45, 888, 101, 11448, 4, 24, 18, 127, 2574, 4, 939, 129, 101, 383, 14, 939, 524, 205, 23, 8, 939, 524, 45, 190, 1402, 939, 74, 101, 7, 28, 205, 23, 11448, 98, 939, 56, 127, 13747, 77, 145, 4036, 7, 6713, 19, 5, 9743, 950, 1159, 53, 939, 657, 209, 1159, 98, 734, 22, 44412, 24, 6, 22633, 6, 905, 18, 213, 11448, 72, 939, 109, 33, 7, 8109, 939, 269, 56, 10, 7814, 259, 4, 24, 16, 12669, 8, 2850, 22383, 15, 5, 751, 61, 7, 162, 16, 95, 10, 11448, 18743, 145, 5322, 19, 1495, 4, 24, 1326, 98, 793, 8, 33676, 219, 31, 5, 751, 14, 47, 32, 2829, 551, 36347, 30, 5, 8408, 4259, 4, 24, 18, 10, 828, 101, 47, 33, 393, 450, 65, 137, 8, 26388, 965, 75, 806, 2770, 116, 939, 33, 7, 224, 5, 2003, 544, 21, 10, 410, 2635, 4, 51, 39187, 227, 130, 36399, 13563, 8, 132, 155, 9, 106, 58, 15983, 98, 939, 40, 905, 24, 7117, 4, 5, 371, 15, 5, 97, 865, 21, 1153, 357, 14756, 13, 127, 37179, 25684, 6, 2225, 3784, 633, 4, 5, 16328, 58, 959, 132, 13, 10, 1897, 11, 10, 1275, 385, 24006, 4946, 4, 24, 9180, 162, 9, 5, 360, 939, 115, 120, 10, 1275, 385, 24006, 4946, 24432, 132, 19, 481, 4885, 5622, 70, 363, 4, 10, 36646, 1564, 360, 734, 52, 58, 259, 13, 30837, 11448, 4, 24, 21, 6152, 198, 316, 4, 939, 5844, 75, 7323, 1329, 11, 1153, 195, 50, 231, 107, 8, 939, 524, 1256, 686, 939, 1199, 2330, 14, 172, 4, 372, 850, 4, 5, 29831, 21, 17653, 103, 1256, 4206, 19764, 543, 6790, 6287, 98, 939, 64, 202, 224, 22, 36613, 113, 235, 116, 127, 2674, 233, 9, 5, 1559, 439, 402, 101, 42, 4, 162, 22, 9226, 29831, 16, 816, 103, 1256, 7297, 2682, 6, 53, 939, 206, 37, 34, 769, 702, 10, 367, 3686, 187, 52, 348, 57, 259, 4, 24438, 1116, 31411, 72, 1441, 22, 6025, 18, 740, 196, 4063, 740, 3209, 1250, 366, 72, 162, 22, 12196, 18, 740, 196, 4063, 740, 3209, 1250, 366, 1917, 1441, 22, 627, 29831, 72, 162, 1326, 2789, 23, 29831, 22, 44127, 15328, 5, 29831, 16, 740, 196, 4063, 23523, 740, 3209, 1250, 366, 2901, 18820, 5749, 16, 1256, 37689, 593, 7, 2230, 99, 10, 11448, 18743, 197, 28, 8, 2145, 22, 9226, 16, 45, 128, 8697, 4, 42, 16, 11448, 4, 89, 32, 1492, 72, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcqROsA5aYO7",
        "outputId": "29674ea1-15ec-46eb-a6b9-3290e2df9b38"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in train_input_ids]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  1223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqzAIiPdar8J",
        "outputId": "ec08302f-b4d2-45da-9081-ce5f4f3e297d"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# Set the maximum sequence length.\n",
        "\n",
        "MAX_LEN = 256\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "# Pad our input tokens with value 0.\n",
        "\n",
        "train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print('\\Done.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 256 values...\n",
            "\n",
            "Padding token: \"<pad>\", ID: 1\n",
            "\\Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-WBqHusax93"
      },
      "source": [
        "# Create attention masks\n",
        "train_attention_masks = []\n",
        "# For each sentence...\n",
        "for sent in train_input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    train_attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NafVfKLPa1D1",
        "outputId": "c68fc2e8-546c-4ac1-f579-b644d28f35f8"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "valid_input_ids = []\n",
        "for sent in valid_text:\n",
        "\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        \n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    valid_input_ids.append(encoded_sent)\n",
        "\n",
        "print('Max sentence length: ', max([len(sen) for sen in valid_input_ids]))\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 256\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "# Pad our input tokens with value 0.\n",
        "\n",
        "valid_input_ids = pad_sequences(valid_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print('\\Done.')\n",
        "\n",
        "# Create attention masks\n",
        "valid_attention_masks = []\n",
        "# For each sentence...\n",
        "for sent in valid_input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    valid_attention_masks.append(att_mask)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  food is sometimes sometimes great and somethings just good. wasn't a fan of their brunch but lunch and dinner is good. service is hit and miss.\n",
            "Token IDs: [0, 13193, 16, 2128, 2128, 372, 8, 45420, 298, 1033, 95, 205, 4, 938, 75, 10, 2378, 9, 49, 25003, 53, 4592, 8, 3630, 16, 205, 4, 544, 16, 478, 8, 2649, 4, 2]\n",
            "Max sentence length:  1108\n",
            "\n",
            "Padding/truncating all sentences to 256 values...\n",
            "\n",
            "Padding token: \"<pad>\", ID: 1\n",
            "\\Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUjxkre6a9hy"
      },
      "source": [
        "train_inputs, valid_inputs = train_input_ids, valid_input_ids\n",
        "train_masks, valid_masks = train_attention_masks, valid_attention_masks\n",
        "train_labels, valid_labels = train_label, valid_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm9NNlL-cJm2"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(valid_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(valid_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(valid_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ekwYBR-cM82"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI0Q-u8-cVyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4b0c5c-4dd1-4b33-a727-c9aa937746d2"
      },
      "source": [
        "from transformers import RobertaForSequenceClassification, AdamW, RobertaConfig\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 5, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvMj5Iphci5U",
        "outputId": "6b6444d1-f66b-4443-81d2-66af4f714711"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "roberta.embeddings.word_embeddings.weight               (50265, 768)\n",
            "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
            "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
            "roberta.embeddings.LayerNorm.weight                           (768,)\n",
            "roberta.embeddings.LayerNorm.bias                             (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
            "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
            "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
            "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
            "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
            "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
            "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "classifier.dense.weight                                   (768, 768)\n",
            "classifier.dense.bias                                         (768,)\n",
            "classifier.out_proj.weight                                  (5, 768)\n",
            "classifier.out_proj.bias                                        (5,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmwOKXFIcqfO"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 0.000015, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuB79y1yczVB"
      },
      "source": [
        "import numpy as np\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9fexYY5c2Cq"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_FtD5zxc9hK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d1a2e1-f3d9-4bed-8563-0ffda100a540"
      },
      "source": [
        "\n",
        "loss_values = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()        \n",
        "        \n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        if step % 20 == 19:\n",
        "          print(loss)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        \n",
        "        logits = outputs[0]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "        \n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "tensor(1.4989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(1.5424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch    40  of    625.    Elapsed: 0:00:31.\n",
            "tensor(1.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(1.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch    80  of    625.    Elapsed: 0:01:02.\n",
            "tensor(0.8620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.9292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   120  of    625.    Elapsed: 0:01:33.\n",
            "tensor(0.8782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   160  of    625.    Elapsed: 0:02:03.\n",
            "tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   200  of    625.    Elapsed: 0:02:34.\n",
            "tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.9106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   240  of    625.    Elapsed: 0:03:05.\n",
            "tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.8650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   280  of    625.    Elapsed: 0:03:36.\n",
            "tensor(0.8089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.8079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   320  of    625.    Elapsed: 0:04:06.\n",
            "tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   360  of    625.    Elapsed: 0:04:37.\n",
            "tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.8502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   400  of    625.    Elapsed: 0:05:08.\n",
            "tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.4943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   440  of    625.    Elapsed: 0:05:39.\n",
            "tensor(0.5931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.7100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   480  of    625.    Elapsed: 0:06:10.\n",
            "tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   520  of    625.    Elapsed: 0:06:40.\n",
            "tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   560  of    625.    Elapsed: 0:07:11.\n",
            "tensor(0.8492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   600  of    625.    Elapsed: 0:07:42.\n",
            "tensor(0.8855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "\n",
            "  Average training loss: 0.81\n",
            "  Training epcoh took: 0:08:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.71\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch    40  of    625.    Elapsed: 0:00:31.\n",
            "tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch    80  of    625.    Elapsed: 0:01:02.\n",
            "tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   120  of    625.    Elapsed: 0:01:32.\n",
            "tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   160  of    625.    Elapsed: 0:02:03.\n",
            "tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   200  of    625.    Elapsed: 0:02:34.\n",
            "tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   240  of    625.    Elapsed: 0:03:05.\n",
            "tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   280  of    625.    Elapsed: 0:03:35.\n",
            "tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.4999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   320  of    625.    Elapsed: 0:04:06.\n",
            "tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   360  of    625.    Elapsed: 0:04:37.\n",
            "tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.4825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   400  of    625.    Elapsed: 0:05:08.\n",
            "tensor(0.7256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.8856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   440  of    625.    Elapsed: 0:05:39.\n",
            "tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.4141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   480  of    625.    Elapsed: 0:06:09.\n",
            "tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.4568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   520  of    625.    Elapsed: 0:06:40.\n",
            "tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   560  of    625.    Elapsed: 0:07:11.\n",
            "tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   600  of    625.    Elapsed: 0:07:42.\n",
            "tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epcoh took: 0:08:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch    40  of    625.    Elapsed: 0:00:31.\n",
            "tensor(0.3463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch    80  of    625.    Elapsed: 0:01:02.\n",
            "tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   120  of    625.    Elapsed: 0:01:32.\n",
            "tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   160  of    625.    Elapsed: 0:02:03.\n",
            "tensor(0.5499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   200  of    625.    Elapsed: 0:02:34.\n",
            "tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   240  of    625.    Elapsed: 0:03:05.\n",
            "tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   280  of    625.    Elapsed: 0:03:36.\n",
            "tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   320  of    625.    Elapsed: 0:04:06.\n",
            "tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   360  of    625.    Elapsed: 0:04:37.\n",
            "tensor(0.5168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   400  of    625.    Elapsed: 0:05:08.\n",
            "tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   440  of    625.    Elapsed: 0:05:39.\n",
            "tensor(0.7642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   480  of    625.    Elapsed: 0:06:10.\n",
            "tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   520  of    625.    Elapsed: 0:06:40.\n",
            "tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   560  of    625.    Elapsed: 0:07:11.\n",
            "tensor(0.5077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "  Batch   600  of    625.    Elapsed: 0:07:42.\n",
            "tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 0:08:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5GnnZladAnz"
      },
      "source": [
        "# make predictions on test set\n",
        "test_input_ids = []\n",
        "for sent in test_text:\n",
        "    \n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    test_input_ids.append(encoded_sent)\n",
        "# Pad our input tokens\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "test_attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in test_input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  test_attention_masks.append(seq_mask) \n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(test_input_ids)\n",
        "prediction_masks = torch.tensor(test_attention_masks)\n",
        "prediction_labels = torch.tensor([0]*len(test_input_ids))\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP2A-LwaQsDG",
        "outputId": "dd8575bd-370a-4f5f-a8d0-08c643d5f2db"
      },
      "source": [
        "# Prediction on test set\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  # label_ids = b_labels.to('cpu').numpy()\n",
        "  logits = np.argmax(logits, axis=1).flatten()\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits+1)\n",
        "  # true_labels.append(label_ids)\n",
        "print('DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 2,000 test sentences...\n",
            "DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "lNRwovlHMEb1",
        "outputId": "7e9b0215-7761-4117-9f9c-399e39f265bb"
      },
      "source": [
        "import plotly.express as px\n",
        "f = pd.DataFrame(loss_values)\n",
        "f.columns=['Loss']\n",
        "fig = px.line(f, x=f.index, y=f.Loss)\n",
        "fig.update_layout(title='Training loss of the Model',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Loss')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"daaf431b-2c3b-430d-b225-51d850711d7c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"daaf431b-2c3b-430d-b225-51d850711d7c\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'daaf431b-2c3b-430d-b225-51d850711d7c',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"index=%{x}<br>Loss=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0, 1, 2], \"xaxis\": \"x\", \"y\": [0.8104128609657287, 0.634035748052597, 0.5574911930799484], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training loss of the Model\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('daaf431b-2c3b-430d-b225-51d850711d7c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfxuTZSiQ-S3"
      },
      "source": [
        "ans = list(predictions[0])\n",
        "for i in predictions[1:]:\n",
        "  ans.extend(list(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP4y7rXmRMn_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "af1f74fb-1432-4f6a-8e6b-2b8349b67055"
      },
      "source": [
        "(pd.DataFrame({'review_id':testset.review_id, 'stars':ans})).to_csv('test_sample.csv')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c58e7b4c4f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'review_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalidset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stars'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_sample.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    }
  ]
}